Introduction

Prompt injection is a specific vulnerability in language modeling (LLM) where an attacker inserts malicious commands or content into the prompt, manipulating the model's behavior and potentially resulting in unsafe or incorrect responses. This cheat sheet describes the issues and countermeasures to mitigate these risks.

Typical Problem Causes

    Unfiltered or uncontrolled insertion of user input into LLM prompts.

    Lack of isolation between context data and general commands.

    Prompts that are too general or that allow hidden or ambiguous instructions to be interpreted.

    Lack of validation or sanitization of input chained into the prompt.

Solutions and Prevention

    Thoroughly sanitize and validate all data entered into prompts.

    Clearly separate user data from system commands in the prompt (e.g., use strict templates).

    Limit the context provided to the model to minimize the attack surface.

    Monitor and log prompts and responses to identify injection attempts.

    Use prompt engineering techniques to reduce ambiguity and unwanted interpretations.

    Implement filtering and control mechanisms for generated responses.